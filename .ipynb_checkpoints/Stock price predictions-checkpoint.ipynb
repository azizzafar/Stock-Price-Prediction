{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58291380",
   "metadata": {},
   "source": [
    "Group name: 16\n",
    "\n",
    "Project title: Stock Price Prediction\n",
    "\n",
    "Members:\n",
    "*Aziz Zafar*, *Ali Akbar Rehman*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install yfinance\n",
    "!pip install pandas-datareader\n",
    "!pip install pandas==1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76625ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install statsmodels\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "yf.pdr_override() \n",
    "import pandas as pd\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e546907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation \n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, LeakyReLU, MaxPooling1D,Flatten, GlobalAveragePooling1D, Dense, Dropout, AveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b8c02",
   "metadata": {},
   "source": [
    "### Project description\n",
    "Stock market analysis: Finding the seasonality and trend\n",
    "\n",
    "In this project you will investigate several stocks of your choosing to compare their seasonality and trend. Try choosing some stocks that are seasonal, like airlines, air conditioning, heating, etc. Decompose the stocks that you choose and discuss their seasonality and trend. Using one or more ARIMA models, try to predict the future price of the stock. The data is gathered from yahoo Finance, and can be accessed throug the yahoo-finance python library (https://pypi.org/project/yahoo-finance/Links to an external site.).\n",
    "\n",
    "https://github.com/aliakbarrehman/DAT550_Data_Mining/tree/master/project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde41a5",
   "metadata": {},
   "source": [
    "#### Stocks Chosen\n",
    "We have chosen the following stocks for seasonality analysis and prediction.\n",
    "- Equinor (EQNR) Currency: USD\n",
    "- Marriot Hotel (MAR) Currency: USD\n",
    "- Microsoft (MSFT) Currency: USD\n",
    "- Nestle (NESN) Currency: CHF\n",
    "- Turkish Airlines - Türk Hava Yolları A.O. (THYAO) Currency: TRY\n",
    "\n",
    "First of all we will get historical data for the stocks and save them in csv files so as to avoid calling Yahoo API over and over again, since Yahoo has introduced a number of measures to stop scrappers and reduce usage of the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(tickers, start_date, end_date):\n",
    "    for ticker in tickers:\n",
    "        print('Getting ' + ticker + ' stock data. Period: ' + start_date + ' - ' + end_date + ' Interval: Daily')\n",
    "        data = pdr.get_data_yahoo(ticker, interval='1d', start=start_date, end=end_date)\n",
    "        filename = './data/' + ticker + '_' + start_date + '-' + end_date + '.csv'\n",
    "        print('Saving Data. Path: ' + filename)\n",
    "        # Reindex using date as index\n",
    "        print(data.head())\n",
    "        data.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2766c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list=['EQNR', 'MAR', 'MSFT', 'NESN.SW', 'THYAO.IS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68325e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get data by our choice by giving days bracket\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2022-12-31'\n",
    "get_stock_data(ticker_list, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbfd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqnr = pd.read_csv('./data/EQNR_2013-01-01-2022-12-31.csv', index_col = 0)\n",
    "mar = pd.read_csv('./data/MAR_2013-01-01-2022-12-31.csv', index_col = 0)\n",
    "msft = pd.read_csv('./data/MSFT_2013-01-01-2022-12-31.csv', index_col = 0)\n",
    "nesn = pd.read_csv('./data/NESN.SW_2013-01-01-2022-12-31.csv', index_col = 0)\n",
    "thyao = pd.read_csv('./data/THYAO.IS_2013-01-01-2022-12-31.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqnr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd32e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting closing values of stocks\n",
    "eqnr['Close'].plot(label = 'Equinor Close', figsize = (16, 10))\n",
    "mar['Close'].plot(label = 'Marriot Close')\n",
    "msft['Close'].plot(label = 'Microsoft Close')\n",
    "nesn['Close'].plot(label = 'Nestle Close')\n",
    "thyao['Close'].plot(label = 'Turkish Airlines Close')\n",
    "plt.legend()\n",
    "plt.ylabel('Stock Closing Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d41cc",
   "metadata": {},
   "source": [
    "We see that Roughly all the stocks have roughly the same trend. From 2013 uptill 2018 the stock values follow a flat trend but after 2018 all the stocks tend to increase in value. Let's plot the volume of the stocks and see if that could have had an impact on the values increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting volume of stocks\n",
    "eqnr['Volume'].plot(label = 'Equinor Volume', figsize = (16, 10))\n",
    "mar['Volume'].plot(label = 'Marriot Volume')\n",
    "msft['Volume'].plot(label = 'Microsoft Volume')\n",
    "nesn['Volume'].plot(label = 'Nestle Volume')\n",
    "thyao['Volume'].plot(label = 'Turkish Airlines Volume')\n",
    "plt.legend()\n",
    "plt.ylabel('Stock Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44e8bd",
   "metadata": {},
   "source": [
    "This graph does not help alot to visualize how valuable a stock is so we will add a Traded Volume column to the dataframe to help us visualize the value of a stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqnr['Traded'] = eqnr['Close'] * eqnr['Volume']\n",
    "mar['Traded'] = mar['Close'] * mar['Volume']\n",
    "msft['Traded'] = msft['Close'] * msft['Volume']\n",
    "nesn['Traded'] = nesn['Close'] * nesn['Volume']\n",
    "thyao['Traded'] = thyao['Close'] * thyao['Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Traded Volume\n",
    "eqnr['Traded'].plot(label = 'Equinor Trade', figsize = (16, 10))\n",
    "mar['Traded'].plot(label = 'Marriot Trade')\n",
    "msft['Traded'].plot(label = 'Microsoft Trade')\n",
    "nesn['Traded'].plot(label = 'Nestle Trade')\n",
    "thyao['Traded'].plot(label = 'Turkish Airlines Trade')\n",
    "plt.legend()\n",
    "plt.ylabel('Volume Traded')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fd740",
   "metadata": {},
   "source": [
    "#### Pre-Processing\n",
    "1. First of all we resample values to fill in any missing day values. We use linear interpolation to fill in the data after resampling\n",
    "2. We will only be working with closing prices of the stocks so creating a dataframe with closing prices of the 5 stocks we have chosen\n",
    "\n",
    "We need to some model specific pre processing which we will do in the section for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_daily(df):\n",
    "    # Index using datetime to be able to plot seasonality and trend\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    # Resample to fill in missing days in the timeseries\n",
    "    # Using linear interpolation to fill in the missing values\n",
    "    return df.resample('1D').asfreq().interpolate(method = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037951e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal_decompose method can't accept gaps in the dataset. So we will fill the missing days with linear interpolation\n",
    "eqnr = resample_daily(eqnr)\n",
    "mar = resample_daily(mar)\n",
    "msft = resample_daily(msft)\n",
    "nesn = resample_daily(nesn)\n",
    "thyao = resample_daily(thyao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d17091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking average of selected stocks to build a trend for the market\n",
    "overAllTrend = pd.DataFrame([eqnr['Close'], mar['Close'], msft['Close'], nesn['Close'], thyao['Close']]).T\n",
    "# Calculate rolling mean of adjacent values\n",
    "rolling_mean = overAllTrend.rolling(window=10, min_periods=1, center=True).mean()\n",
    "# Fill NaN values with rolling mean\n",
    "overAllTrend.fillna(rolling_mean, inplace=True)\n",
    "overAllTrend = overAllTrend.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77149d",
   "metadata": {},
   "source": [
    "## Trend Added into the closing_price DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ffa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_price = pd.DataFrame()\n",
    "closing_price['EQNR'] = eqnr['Close']\n",
    "closing_price['MAR'] = mar['Close']\n",
    "closing_price['MSFT'] = msft['Close']\n",
    "closing_price['NESN.SW'] = nesn['Close']\n",
    "closing_price['THYAO.IS'] = thyao['Close']\n",
    "closing_price['Trend'] = overAllTrend\n",
    "closing_price = closing_price.fillna(0)\n",
    "closing_price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28274e4",
   "metadata": {},
   "source": [
    "### Time Series Components\n",
    "\n",
    "First of all we will analyze time series's componenets. Most time series data can be broken down into 2 kinds of components.\n",
    "- Systematic components are components that can be modelled. Systematic components of the Stock data are Trend and Seasonality.\n",
    "- Non-Systematic components that cannot be modeled like noise. Stock data contains a lot of noise which can be variation to the stock prices from external factors like global phenomena not accounted for in the model.\n",
    "\n",
    "There are 2 kinds of models to break a time series into components.\n",
    "- Additive Model: This is the model where any point in the time series the value can be represented by a sum of components.\n",
    "- Multiplicative Model: This is the model where any point in the series is represented by a product of the components.\n",
    "\n",
    "We tried both models for decomposition of stock data and both presented similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def decompose_into_seasonality_and_trend(series, model = 'additive', period = 90):\n",
    "    series_components = seasonal_decompose(series, model = model, period = period)\n",
    "    return [series_components.trend, series_components.seasonal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[eqnr_trend, eqnr_seasonality] = decompose_into_seasonality_and_trend(eqnr['Close'])\n",
    "[mar_trend, mar_seasonality] = decompose_into_seasonality_and_trend(mar['Close'])\n",
    "[msft_trend, msft_seasonality] = decompose_into_seasonality_and_trend(msft['Close'])\n",
    "[nesn_trend, nesn_seasonality] = decompose_into_seasonality_and_trend(nesn['Close'])\n",
    "[thyao_trend, thyao_seasonality] = decompose_into_seasonality_and_trend(thyao['Close'])\n",
    "\n",
    "eqnr_trend.plot(label = 'Equinor Trend', figsize = (16, 10))\n",
    "mar_trend.plot(label = 'Marriot Trend')\n",
    "msft_trend.plot(label = 'Microsoft Trend')\n",
    "nesn_trend.plot(label = 'Nestle Trend')\n",
    "thyao_trend.plot(label = 'Turkish Airlines Trend')\n",
    "plt.legend()\n",
    "plt.ylabel('Trend')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "eqnr_seasonality.plot(label = 'Equinor Seasonality', figsize = (16, 10))\n",
    "plt.show()\n",
    "mar_seasonality.plot(label = 'Marriot Seasonality', figsize = (16, 10))\n",
    "plt.show()\n",
    "msft_seasonality.plot(label = 'Microsoft Seasonality', figsize = (16, 10))\n",
    "plt.show()\n",
    "nesn_seasonality.plot(label = 'Nestle Seasonality', figsize = (16, 10))\n",
    "plt.show()\n",
    "thyao_seasonality.plot(label = 'Turkish Airlines Seasonality', figsize = (16, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59528c72",
   "metadata": {},
   "source": [
    "From the above graphs we can not read much information from the seasonality decomposition because of a high level of noise in the data so we will break it down and compute seasonality for one year at a time and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_yearly_seasonality(series, start = 2015, end = 2018):\n",
    "    from_index = 0\n",
    "    to_index = 365\n",
    "    for i in range(start, end):\n",
    "        [_, seasonality] = decompose_into_seasonality_and_trend(series[from_index:to_index])\n",
    "        from_index = to_index\n",
    "        to_index = to_index + 365\n",
    "        seasonality.plot(label = str(i), figsize = (16, 10))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358106e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_yearly_seasonality(closing_price['EQNR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46404fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_yearly_seasonality(closing_price['MAR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f47509",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549d213",
   "metadata": {},
   "source": [
    "For ARIMA, time series has to be made stationary for further analysis. What is stationarity and why is it important in the analysis of time series data? In simple words stationarity mean that the statistical properties of the process that outputs the time series data does not change as we progress further in the time series. Or that as the time series changes the way it changes stays consistent. Noise presents a problem in the stationarity of the time series normally. \n",
    "\n",
    "So following the above hypothesis we want the properties like mean and variance to stay consistent throughout the series. A stationary time series will have no long-term predictable patterns such as trends or seasonality. Time plots will show the series to roughly have a horizontal trend with the constant variance.\n",
    "\n",
    "ARIMA model requires the time series data to be stationary. So we normalize the dataset so that the stock data becomes stationary. To test for stationarity we use Augmented Dicky-Fuller (ADF) which states that if the p-value is < 0.05 then the data is stationary otherwise the data is non stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def is_series_stationary(stock_close_price):\n",
    "    print(\"ADF Test:\")\n",
    "    adf_test = adfuller(stock_close_price, autolag = 'AIC')\n",
    "    print('p-value: %f' % adf_test[1])\n",
    "\n",
    "for i in ticker_list:\n",
    "    closing_price[i].plot(figsize = (16, 10))\n",
    "    plt.show()\n",
    "    print(i + ' ADF: ')\n",
    "    is_series_stationary(closing_price[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c58a21",
   "metadata": {},
   "source": [
    "As we can see that all the time series have p > 0.05 meaning that all the time series are not-stationary. We can make our time series stationary by using a method called differencing. In differencing a new series is calculated by calculating the value at the current time by differencing the value of actual observation of current time and its previous time (n).\n",
    "\n",
    "```y(t) = y(t) - y(t - n)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc85fd",
   "metadata": {},
   "source": [
    "##### Model Building\n",
    "ARIMA model needs 3 parameters\n",
    "1. p which is the order of the auto-regressive model.\n",
    "2. d which is the difference order.\n",
    "3. q which is the order of the moving average model.\n",
    "\n",
    "To find out the optimal values for our ARIMA model we will look at the Autocorrelation plots and the Partial Autocorrelation plots to find out the values for p, d and q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ticker_list:\n",
    "    plot_acf(closing_price[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Order 1 differencing\n",
    "closing_price_1n = closing_price.diff().dropna()\n",
    "\n",
    "for i in ticker_list:\n",
    "    plot_acf(closing_price_1n[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611297ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "for i in ticker_list:\n",
    "    plot_pacf(closing_price_1n[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(closing_price_1n['EQNR'], order = (1, 2, 2))\n",
    "model_results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_diff_pred = pd.Series(model_results.fittedvalues, copy=True) # converting the fitted values of the results into series\n",
    "ARIMA_pred_cumsum = ARIMA_diff_pred.cumsum() # calculating the cumulative sum\n",
    "ARIMA_pred = pd.Series(closing_price['EQNR'].iloc[0], index = closing_price['EQNR'].index)\n",
    "ARIMA_pred = ARIMA_pred.add(ARIMA_pred_cumsum, fill_value = 0) # adding the cumulative sum to the differenced data to cancel out the differencing effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed46c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(closing_price['EQNR'])\n",
    "plt.plot(ARIMA_pred)\n",
    "plt.legend(labels=['Original Stock Value','ARIMA Predicted Stock Value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(closing_price_1n['MAR'], order = (1, 2, 2))\n",
    "model_results = model.fit()\n",
    "\n",
    "ARIMA_diff_pred = pd.Series(model_results.fittedvalues, copy=True) # converting the fitted values of the results into series\n",
    "ARIMA_pred_cumsum = ARIMA_diff_pred.cumsum() # calculating the cumulative sum\n",
    "ARIMA_pred = pd.Series(closing_price['MAR'].iloc[0], index = closing_price['MAR'].index)\n",
    "ARIMA_pred = ARIMA_pred.add(ARIMA_pred_cumsum, fill_value = 0) # adding the cumulative sum to the differenced data to cancel out the differencing effect\n",
    "\n",
    "plt.plot(closing_price['MAR'])\n",
    "plt.plot(ARIMA_pred)\n",
    "plt.legend(labels=['Original Stock Value','ARIMA Predicted Stock Value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(closing_price_1n['MSFT'], order = (1, 2, 2))\n",
    "model_results = model.fit()\n",
    "\n",
    "ARIMA_diff_pred = pd.Series(model_results.fittedvalues, copy=True) # converting the fitted values of the results into series\n",
    "ARIMA_pred_cumsum = ARIMA_diff_pred.cumsum() # calculating the cumulative sum\n",
    "ARIMA_pred = pd.Series(closing_price['MSFT'].iloc[0], index = closing_price['MSFT'].index)\n",
    "ARIMA_pred = ARIMA_pred.add(ARIMA_pred_cumsum, fill_value = 0) # adding the cumulative sum to the differenced data to cancel out the differencing effect\n",
    "\n",
    "plt.plot(closing_price['MSFT'],label='original')\n",
    "plt.plot(ARIMA_pred)\n",
    "plt.title('Micorsoft')\n",
    "plt.legend(labels=['Original Stock Value','ARIMA Predicted Stock Value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(closing_price_1n['NESN.SW'], order = (1, 1, 0))\n",
    "model_results = model.fit()\n",
    "\n",
    "ARIMA_diff_pred = pd.Series(model_results.fittedvalues, copy=True) # converting the fitted values of the results into series\n",
    "ARIMA_pred_cumsum = ARIMA_diff_pred.cumsum() # calculating the cumulative sum\n",
    "ARIMA_pred = pd.Series(closing_price['NESN.SW'].iloc[0], index = closing_price['MSFT'].index)\n",
    "ARIMA_pred = ARIMA_pred.add(ARIMA_pred_cumsum, fill_value = 0) # adding the cumulative sum to the differenced data to cancel out the differencing effect\n",
    "\n",
    "plt.plot(closing_price['NESN.SW'],label='original')\n",
    "plt.plot(ARIMA_pred)\n",
    "plt.legend(labels=['Original Stock Value','ARIMA Predicted Stock Value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(closing_price_1n['THYAO.IS'], order = (1, 3, 3))\n",
    "model_results = model.fit()\n",
    "\n",
    "ARIMA_diff_pred = pd.Series(model_results.fittedvalues, copy=True) # converting the fitted values of the results into series\n",
    "ARIMA_pred_cumsum = ARIMA_diff_pred.cumsum() # calculating the cumulative sum\n",
    "ARIMA_pred = pd.Series(closing_price['THYAO.IS'].iloc[0], index = closing_price['MSFT'].index)\n",
    "ARIMA_pred = ARIMA_pred.add(ARIMA_pred_cumsum, fill_value = 0) # adding the cumulative sum to the differenced data to cancel out the differencing effect\n",
    "\n",
    "plt.plot(closing_price['THYAO.IS'],label='original')\n",
    "plt.plot(ARIMA_pred)\n",
    "plt.legend(labels=['Original Stock Value','ARIMA Predicted Stock Value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e88a34",
   "metadata": {},
   "source": [
    "# Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class model:\n",
    "    def __init__(self, closing_price):\n",
    "        \"\"\"\n",
    "        To use LSTM and 1D CNN models for stock price prediction, \n",
    "        data is first converted into a format suitable for use with the models. \n",
    "        The data is divided into fixed-length windows wich are then used as \n",
    "        input to the models. The model then learn to identify patterns in \n",
    "        these windows and make predictions based on them. \n",
    "        \n",
    "        parameter: \n",
    "                closing_price: DataFrame of closing price of stocks\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.closing_price = closing_price  # DataFrame of selected stock prices at close time  \n",
    "        \n",
    "        self.train_set_size = math.ceil(closing_price.shape[0] * 0.8) # training size of data\n",
    "        self.ticker_list=['EQNR', 'MAR', 'MSFT', 'NESN.SW', 'THYAO.IS'] # list of stocks\n",
    "        \n",
    "        # list of train data for stock companies\n",
    "        self.x_train_data = [] \n",
    "        self.y_train_data = []\n",
    "\n",
    "        # list of test data for stock companies\n",
    "        self.x_test_data = []\n",
    "        self.y_test_data = []\n",
    "\n",
    "        # list of scaler objects for each stock\n",
    "        self.scalar_objects = []\n",
    "        for i in self.ticker_list:\n",
    "            # initialize scaler\n",
    "            scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "            scaled_data = scaler.fit_transform(closing_price[i].values.reshape(-1, 1))\n",
    "            train_data = scaled_data[0: self.train_set_size, :]\n",
    "\n",
    "            # adding scaler object into the list (scaler)\n",
    "            self.scalar_objects.append(scaler)\n",
    "            x_train = []\n",
    "            y_train = []\n",
    "\n",
    "            # Create a 60-days window of historical prices (i-60) as our feature data (x_train)\n",
    "            # and the following 60-days window as label data (y_train).\n",
    "            window = 60 # window size \n",
    "            for j in range(window, len(train_data)):\n",
    "                x_train.append(train_data[j - window:j, 0])\n",
    "                y_train.append(train_data[j, 0])\n",
    "\n",
    "            x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "            \n",
    "            # Resahaping x_train dataset: LSTM network and CNN requires the input to be in  3d in the \n",
    "            #form of numbr of samples,number of time steps and number of featurs. [sample, time steps, feature]\n",
    "            x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "            self.x_train_data.append(x_train)\n",
    "            self.y_train_data.append(y_train)\n",
    "\n",
    "            # Test data\n",
    "            test_data = scaled_data[self.train_set_size - window: , :]\n",
    "            x_test = []\n",
    "            y_test = closing_price[i].values[self.train_set_size:]\n",
    "\n",
    "            for k in range(window, len(test_data)):\n",
    "                x_test.append(test_data[k - window:k, 0])\n",
    "\n",
    "            x_test = np.array(x_test)\n",
    "            # Resahaping x_test dataset: LSTM network and CNN requires the input to be in  3d in the\n",
    "            #form of numbr of samples, number of time steps and number of featurs. [sample, time steps, feature]\n",
    "            x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "            self.x_test_data.append(x_test)\n",
    "            self.y_test_data.append(y_test)\n",
    "            \n",
    "    def buildLSTM_Model(self, X):\n",
    "        \"\"\"\n",
    "        parameter: \n",
    "                X: ndArray\n",
    "                \n",
    "        return: model\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(100, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "        model.add(LSTM(100, return_sequences=False))\n",
    "        model.add(Dense(25))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def buildCNN_Model(self, X):\n",
    "        \"\"\"\n",
    "        parameter: \n",
    "                X: ndArray\n",
    "            \n",
    "        return: model\n",
    "        \"\"\"\n",
    "        clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=32, kernel_size=6, activation='relu', input_shape=(X.shape[1], 1)))\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def plot_results(self, epochs, stock_name = None, cnn = False, lstm = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        parameters:\n",
    "                epochs: float: number of epochs for the model \n",
    "                stock_name: stock name from the list ['EQNR', 'MAR', 'MSFT', 'NESN.SW', 'THYAO.IS'] \n",
    "                            to be passed.\n",
    "                cnn: if True, the 1D CNN model will be used for prediction\n",
    "                lstm: if True, the LSTM model will be used for prediction\n",
    "        \n",
    "        \"\"\"\n",
    "        # Equinor ASA data\n",
    "        if stock_name == 'EQNR':\n",
    "            X_train = self.x_train_data[0] # ndArray\n",
    "            y_train = self.y_train_data[0] # list\n",
    "\n",
    "            X_test = self.x_test_data[0] # ndArray\n",
    "            y_test = self.y_test_data[0] # list\n",
    "            \n",
    "            scaler = self.scalar_objects[0] # scaler object\n",
    "            \n",
    "            plot_title = ' Equinor'  # title for the plot\n",
    "            \n",
    "            \n",
    "        # Marriott International data\n",
    "        elif stock_name == 'MAR':\n",
    "            X_train = self.x_train_data[1] # ndArray\n",
    "            y_train = self.y_train_data[1] # list\n",
    "            \n",
    "            X_test = self.x_test_data[1] # ndArray\n",
    "            y_test = self.y_test_data[1] # list\n",
    "            \n",
    "            scaler = self.scalar_objects[1] \n",
    "            \n",
    "            plot_title = 'Marriot Hotel' # title for the plot\n",
    "        \n",
    "        # Microsoft data\n",
    "        elif stock_name == 'MSFT':\n",
    "            X_train = self.x_train_data[2] # ndArray\n",
    "            y_train = self.y_train_data[2] # list\n",
    "            \n",
    "            X_test = self.x_test_data[2] # ndArray\n",
    "            y_test = self.y_test_data[2] # list\n",
    "            \n",
    "            scaler = self.scalar_objects[2]\n",
    "            \n",
    "            plot_title = 'Microsoft' # title for the plot\n",
    "            \n",
    "        # Nestle data    \n",
    "        elif stock_name == 'NESN.SW':\n",
    "            X_train = self.x_train_data[3] # ndArray\n",
    "            y_train = self.y_train_data[3] # list\n",
    "            \n",
    "            X_test = self.x_test_data[3] # ndArray\n",
    "            y_test = self.y_test_data[3] # list\n",
    "\n",
    "            scaler = self.scalar_objects[3]\n",
    "            \n",
    "            plot_title = 'Nestle' # title for the plot\n",
    "        \n",
    "        # Turkish Airlines data\n",
    "        elif stock_name == 'THYAO.IS':\n",
    "            X_train = self.x_train_data[4] # ndArray\n",
    "            y_train = self.y_train_data[4] # list\n",
    "            \n",
    "            X_test = self.x_test_data[4] # ndArray\n",
    "            y_test = self.y_test_data[4] # list\n",
    "            scaler = self.scalar_objects[4]\n",
    "            \n",
    "            plot_title = 'Turkish Airlines' # title for the plot\n",
    "            \n",
    "        else:\n",
    "            pass \n",
    "        \n",
    "        \n",
    "        if cnn == True:\n",
    "            model = self.buildCNN_Model(X_train)  # CNN model  \n",
    "        elif lstm == True:\n",
    "            model = self.buildLSTM_Model(X_train) # LSTM model\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # fit model with the data\n",
    "        model.fit(X_train, y_train, batch_size = 1, epochs = epochs)\n",
    "        # predict with the fitted model\n",
    "        predictions = model.predict(X_test)\n",
    "        # convert the predicted values from scaled form to actual form\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "        # # Getting the root mean squared error (RMSE)\n",
    "        rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
    "        # training data\n",
    "        train = closing_price[stock_name][:self.train_set_size]\n",
    "        # validation data\n",
    "        validation = pd.DataFrame()\n",
    "        validation['Close'] = closing_price[stock_name][self.train_set_size:]\n",
    "        # \n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        predictions_df.index = closing_price[stock_name][self.train_set_size:].index\n",
    "        validation['Predictions'] = predictions_df\n",
    "        plt.figure(figsize=(16,10))\n",
    "        plt.title(plot_title)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Close Price USD ($)')\n",
    "        plt.plot(train)\n",
    "        plt.plot(validation['Close'])\n",
    "        plt.plot(validation['Predictions'])\n",
    "        #plt.plot(closing_price['Trend'])\n",
    "        plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "        plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model(closing_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc1656",
   "metadata": {},
   "source": [
    "## Stock price prediction by CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd73ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 2, stock_name = 'EQNR', cnn = True, lstm =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79976028",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 2, stock_name = 'MAR', cnn = True, lstm =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26dc7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 2, stock_name = 'MSFT', cnn = True, lstm =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee564baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 2, stock_name = 'NESN.SW', cnn = True, lstm =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 2, stock_name = 'THYAO.IS', cnn = True, lstm =  False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af5bd64",
   "metadata": {},
   "source": [
    "## Stock price prediction by LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 3, stock_name = 'EQNR', cnn = False, lstm =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 3, stock_name = 'MAR', cnn = False, lstm =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 3, stock_name = 'MSFT', cnn = False, lstm =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 2, stock_name = 'NESN.SW', cnn = False, lstm =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.plot_results(epochs = 3, stock_name = 'THYAO.IS', cnn = False, lstm =  True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
